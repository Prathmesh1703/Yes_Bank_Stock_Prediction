{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PROJECT TYPE**\n",
        "Supervised Predictive modelling using Regression techniques.\n",
        "\n",
        "# **Contribution**\n",
        "Prathamesh Santosh Bharsakale\n",
        "\n",
        "# **PROJECT SUMMARY:**\n",
        "Financial forecasting is essential for stock market analysis, investment planning, and risk assessment. This project aims to predict closing prices based on historical data using machine learning techniques. The dataset comprises important financial indicators, including Open, High, Low, Close prices, and Date. The main goal is to create an accurate regression model that effectively generalizes to new data while minimizing prediction errors.\n",
        "\n",
        "# **Data Cleaning & Preprocessing**\n",
        "\n",
        "Before training the models, extensive data cleaning was performed to enhance model performance. This included:\n",
        "\n",
        "Removing outliers using Interquartile Range (IQR) to prevent extreme values from skewing predictions.\n",
        "\n",
        "Handling skewness using log transformation, ensuring normally distributed features.\n",
        "\n",
        "Feature scaling using StandardScaler to maintain consistency across different models.\n",
        "\n",
        "The cleaned dataset was then split into training (80%) and testing (20%) sets for model evaluation.\n",
        "\n",
        "Models Implemented & Performance Analysis\n",
        "\n",
        "Tested multiple regression models to identify the best-performing algorithm:\n",
        "\n",
        "Linear Regression.\n",
        "Decision Tree Regressor.\n",
        "XGBoost Regressor.\n",
        "Hyperparameter Tuning\n",
        "\n",
        "To optimize each model, performed Hyperparameter Tuning (HPT) using GridSearchCV and manual trial-and-error:\n",
        "\n",
        "DTR: Tuned max_depth, min_samples_split, min_samples_leaf\n",
        "XGBR: Tuned learning_rate, n_estimators, max_depth, subsample, colsample_bytree\n",
        "This project demonstrates the power of ML in financial forecasting and lays the foundation for stock price prediction, investment risk analysis, and automated trading systems.\n",
        "\n",
        "# **GITHUB LINK:**\n",
        "\n",
        "# **Problem Statement:**\n",
        "Predicting stock prices is a complex and dynamic challenge influenced by numerous market factors. A significant factor to consider is the 2018 fraud case, which likely caused substantial fluctuations in stock prices. The objective is to analyze historical stock price data and create a model that can deliver reliable estimates of future prices."
      ],
      "metadata": {
        "id": "LRMLX1-Ai85k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "kiCCmsrhkFrm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/data_YesBank_StockPrices.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "8WU3yYHxj8o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "BzMxllzBklQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "OW4id0bWksal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "I-tkUNAHlZA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove outliers using IQR\n",
        "Q1 = data[['Open', 'High', 'Low', 'Close']].quantile(0.25)\n",
        "Q3 = data[['Open', 'High', 'Low', 'Close']].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "data = data[~((data[['Open', 'High', 'Low', 'Close']] < (Q1 - 1.5 * IQR)) |\n",
        "               (data[['Open', 'High', 'Low', 'Close']] > (Q3 + 1.5 * IQR))).any(axis=1)]"
      ],
      "metadata": {
        "id": "6ifDvRyCksY1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Date'] = pd.to_datetime(data['Date'], format='%b-%y') # Extracting Month and Year from Date Column.\n",
        "\n",
        "# Extract month, year\n",
        "data['Month'] = data['Date'].dt.month\n",
        "data['Year'] = data['Date'].dt.year\n",
        "\n",
        "data.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "kdfM5XEDksTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Handle skewness using log transformation\n",
        "data[['Open', 'High', 'Low', 'Close']] = np.log(data[['Open', 'High', 'Low', 'Close']] + 1)\n"
      ],
      "metadata": {
        "id": "C4vDmPjoksRE"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaN values and drop them\n",
        "if data.isnull().sum().sum() > 0:\n",
        "    print(\"NaN values found. Dropping rows with NaN values.\")\n",
        "    data = data.dropna()\n"
      ],
      "metadata": {
        "id": "yvJ8Jxa6mv0b"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Scaling"
      ],
      "metadata": {
        "id": "6h_T0IcksSJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "data[['Open', 'High', 'Low']] = scaler.fit_transform(data[['Open', 'High', 'Low']])"
      ],
      "metadata": {
        "id": "GIhRkGwroUt4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spliting the dataset into train and test"
      ],
      "metadata": {
        "id": "LQxevxpusa2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into features and target variable\n",
        "X = data[['Open', 'High', 'Low']]\n",
        "y = data['Close']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "NDuikCOyorN1"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Linear Regression"
      ],
      "metadata": {
        "id": "G5DxTbxrqFnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Linear Regression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "0SsP_MxzorL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions\n",
        "y_pred_linear = model.predict(X_test)\n",
        "y_pred_linear"
      ],
      "metadata": {
        "id": "ujONPng-orJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing prediction and actual values"
      ],
      "metadata": {
        "id": "fqoZd1q7tLRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test.values, label='Actual Prices', marker='o')\n",
        "plt.plot(y_pred_linear, label='Predicted Prices', marker='x', alpha=0.7)\n",
        "plt.title('Actual vs Predicted Closing Prices (Linear Regression)')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Closing Prices')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JuwgxVPzksLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using another Model"
      ],
      "metadata": {
        "id": "86JXXDcStvhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "3Qg87dhat_ho"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Decision Tree Regression\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions\n",
        "y_pred_tree = model.predict(X_test)\n",
        "y_pred_tree"
      ],
      "metadata": {
        "id": "2oyfZPlbt1Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting predictions vs actual values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test.values, label='Actual Prices', marker='o')\n",
        "plt.plot(y_pred_tree, label='Predicted Prices', marker='x', alpha=0.7)\n",
        "plt.title('Actual vs Predicted Closing Prices (Decision Tree Regression)')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Closing Prices')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_wwGhIs4uLcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculating Metrics"
      ],
      "metadata": {
        "id": "Pg6H40QDyOdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
        "mae_linear = mean_absolute_error(y_test, y_pred_linear)\n",
        "r2_linear = r2_score(y_test, y_pred_linear)\n",
        "\n",
        "mse_tree = mean_squared_error(y_test, y_pred_tree)\n",
        "mae_tree = mean_absolute_error(y_test, y_pred_tree)\n",
        "r2_tree = r2_score(y_test, y_pred_tree)\n",
        "\n",
        "print(f'Linear Regression MSE: {mse_linear}')\n",
        "print(f'Linear Regression MAE: {mae_linear}')\n",
        "print(f'Linear Regression R²: {r2_linear}')\n",
        "\n",
        "print(f'Decision Tree Regression MSE: {mse_tree}')\n",
        "print(f'Decision Tree Regression MAE: {mae_tree}')\n",
        "print(f'Decision Tree Regression R²: {r2_tree}')"
      ],
      "metadata": {
        "id": "HBSc6FHXw3FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Create a confusion-like matrix for regression\n",
        "# 1. Binning the predictions and actual values\n",
        "n_bins = 10  # Number of bins - adjust as needed\n",
        "bins = np.linspace(y.min(), y.max(), n_bins + 1)  # Create bins for the target variable\n",
        "\n",
        "# Bin the predicted and actual values using the same bins\n",
        "y_pred_linear_binned = np.digitize(y_pred_linear, bins)\n",
        "y_test_binned = np.digitize(y_test, bins)\n",
        "y_pred_tree_binned = np.digitize(y_pred_tree, bins)\n",
        "\n",
        "# 2. Create confusion matrices\n",
        "conf_matrix_linear = confusion_matrix(y_test_binned, y_pred_linear_binned, labels=np.arange(1, n_bins + 1))\n",
        "conf_matrix_tree = confusion_matrix(y_test_binned, y_pred_tree_binned, labels=np.arange(1, n_bins + 1))\n",
        "\n",
        "# 3. Plotting the confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Adjust labels to match the number of bins\n",
        "labels = [f'Bin {i}' for i in range(1, n_bins + 1)]\n",
        "\n",
        "ConfusionMatrixDisplay(conf_matrix_linear, display_labels=labels).plot(ax=axes[0], cmap='Blues', values_format='d')\n",
        "axes[0].set_title('Confusion-like Matrix (Linear Regression)')\n",
        "\n",
        "ConfusionMatrixDisplay(conf_matrix_tree, display_labels=labels).plot(ax=axes[1], cmap='Blues', values_format='d')\n",
        "axes[1].set_title('Confusion-like Matrix (Decision Tree Regression)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YKjHfZ1cx97o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Performance Comparison: Linear Regression vs. Decision Tree Regression\n",
        "In this analysis, we compared two regression models—Linear Regression and Decision Tree Regression—on the same financial dataset to predict stock closing prices.\n",
        "\n",
        "#Findings:\n",
        "##Linear Regression:\n",
        "\n",
        "This model demonstrated a better performance in terms of accuracy when predicting closing prices. The Mean Squared Error (MSE) for Linear Regression was significantly lower than that of the Decision Tree model, indicating that Linear Regression provided predictions that were closer to the actual values.\n",
        "##Decision Tree Regression:\n",
        "\n",
        "While Decision Trees can capture non-linear relationships and interactions between features, they tend to overfit the training data, especially with limited data points. This often leads to poorer generalization on unseen data, resulting in higher MSE compared to Linear Regression.\n"
      ],
      "metadata": {
        "id": "NQeTkMt3uX4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion:\n",
        "For this specific dataset, the Linear Regression model outperformed the Decision Tree Regression model in predicting accurate values for stock closing prices. This suggests that the relationships within the data are more linear, making Linear Regression a more suitable choice for this particular forecasting task."
      ],
      "metadata": {
        "id": "z0g57tFl0IjB"
      }
    }
  ]
}